{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10836245,"sourceType":"datasetVersion","datasetId":6729254},{"sourceId":10836295,"sourceType":"datasetVersion","datasetId":6729285},{"sourceId":10849722,"sourceType":"datasetVersion","datasetId":6738365},{"sourceId":266829,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":228332,"modelId":250092}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## HF-Net","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom torch.nn import init\n\ndef image_normalization(image, pixel_value_offset=128.0, pixel_value_scale=128.0):\n    return (image - pixel_value_offset) / pixel_value_scale\n\nclass VLAD(nn.Module):\n    def __init__(self, config):\n        super(VLAD, self).__init__()\n        self.intermediate_proj = config.get('intermediate_proj', None)\n        if self.intermediate_proj:\n            self.pre_proj = nn.Conv2d(240, self.intermediate_proj, kernel_size=1)\n\n        self.n_clusters = config['n_clusters']\n        self.memberships = nn.Conv2d(240, self.n_clusters, kernel_size=1)\n\n        # Cluster centers\n        self.clusters = nn.Parameter(torch.empty(1, self.n_clusters, 240))\n        self._initialize_weights()  # Initialize the cluster weights\n\n    def _initialize_weights(self):\n        # Xavier initialization for cluster weights\n        init.xavier_uniform_(self.clusters)\n\n    def forward(self, feature_map, mask=None):\n        if self.intermediate_proj:\n            feature_map = self.pre_proj(feature_map)\n\n        batch_size, _, h, w = feature_map.size()\n\n        # Compute memberships (soft-assignment)\n        memberships = F.softmax(self.memberships(feature_map), dim=1)\n\n        # Reshape feature_map and clusters for broadcasting\n        feature_map = feature_map.permute(0, 2, 3, 1).unsqueeze(3)  # (B, H, W, 1, D)\n        residuals = self.clusters - feature_map  # Compute residuals\n        residuals = residuals * memberships.permute(0, 2, 3, 1).unsqueeze(4)  # Weight residuals by memberships\n\n        if mask is not None:\n            residuals = residuals * mask.unsqueeze(-1).unsqueeze(-1)\n\n        # Sum residuals to form the VLAD descriptor\n        descriptor = residuals.sum(dim=[1, 2])\n\n        # Intra-normalization\n        descriptor = F.normalize(descriptor, p=2, dim=-1)\n\n        # Flatten descriptor and apply L2 normalization\n        descriptor = descriptor.view(batch_size, -1)\n        descriptor = F.normalize(descriptor, p=2, dim=1)\n\n        return descriptor\n\nclass DimensionalityReduction(nn.Module):\n    def __init__(self, config, proj_regularizer=None):\n        \"\"\"\n        Initializes the Dimensionality Reduction module.\n\n        Args:\n            input_dim (int): Dimension of the input feature descriptor.\n            output_dim (int): Dimension of the reduced descriptor.\n            proj_regularizer (float, optional): L2 regularization strength. If None, no regularization is applied.\n        \"\"\"\n        super(DimensionalityReduction, self).__init__()\n        input_dim = config['n_clusters'] * 240\n        output_dim = config['dimensionality_reduction']\n        self.proj_regularizer = proj_regularizer\n\n        # Fully connected layer with Xavier initialization\n        self.fc = nn.Linear(input_dim, output_dim)\n        nn.init.xavier_uniform_(self.fc.weight)\n\n        # Optional L2 regularization\n        if proj_regularizer is not None:\n            self.regularizer = lambda w: proj_regularizer * torch.sum(w ** 2)\n        else:\n            self.regularizer = None\n\n    def forward(self, descriptor):\n        \"\"\"\n        Forward pass for the Dimensionality Reduction module.\n\n        Args:\n            descriptor (torch.Tensor): Input feature descriptor of shape (batch_size, input_dim).\n\n        Returns:\n            torch.Tensor: Reduced and normalized descriptor of shape (batch_size, output_dim).\n        \"\"\"\n        # Normalize the input descriptor\n        descriptor = F.normalize(descriptor, p=2, dim=-1)\n\n        # Apply the fully connected layer\n        descriptor = self.fc(descriptor)\n\n        # Normalize the output descriptor\n        descriptor = F.normalize(descriptor, p=2, dim=-1)\n\n        # Apply regularization if specified\n        if self.regularizer is not None:\n            reg_loss = self.regularizer(self.fc.weight)\n            return descriptor, reg_loss\n\n        return descriptor\n\n\nclass LocalHead(nn.Module):\n    def __init__(self, config):\n        super(LocalHead, self).__init__()\n        descriptor_dim = config['descriptor_dim']\n        detector_grid = config['detector_grid']\n\n        # Descriptor Head\n        self.desc_conv1 = nn.Conv2d(config['input_channels'], descriptor_dim, kernel_size=3, stride=1, padding=1)\n        self.desc_bn1 = nn.BatchNorm2d(descriptor_dim)\n        self.desc_conv2 = nn.Conv2d(descriptor_dim, descriptor_dim, kernel_size=1, stride=1, padding=0)\n\n        # Detector Head\n        self.det_conv1 = nn.Conv2d(config['input_channels'], 128, kernel_size=3, stride=1, padding=1)\n        self.det_bn1 = nn.BatchNorm2d(128)\n        self.det_conv2 = nn.Conv2d(128, 1 + detector_grid ** 2, kernel_size=1, stride=1, padding=0)\n\n        self.detector_grid = detector_grid\n\n    def forward(self, features):\n        # Descriptor Head\n        desc = F.relu6(self.desc_bn1(self.desc_conv1(features)))\n        desc = self.desc_conv2(desc)\n        desc = F.normalize(desc, p=2, dim=1)\n\n        # Detector Head\n        logits = F.relu6(self.det_bn1(self.det_conv1(features)))\n        logits = self.det_conv2(logits)\n\n        prob_full = F.softmax(logits, dim=1)  # Compute softmax over the last dimension\n        prob = prob_full[:, :-1, :, :]  # Exclude the \"no interest point\" dustbin\n        prob = F.pixel_shuffle(prob, self.detector_grid)  # Convert to dense scores\n        prob = torch.squeeze(prob, dim=1)  # Remove unnecessary channel dimension\n\n        return desc, logits, prob_full, prob\n\n\ndef _make_divisible(v, divisor, min_value=None):\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\ndef conv_3x3_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        hidden_dim = round(inp * expand_ratio)\n        self.identity = stride == 1 and inp == oup\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n\n    def forward(self, x):\n        if self.identity:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass HFNet(nn.Module):\n    def __init__(self, config, width_mult=1.0):\n        super(HFNet, self).__init__()\n        # [expand_ratio, channels, repeats, stride]\n        self.cfgs = [\n            [1,  16, 1, 1],\n            [6,  24, 2, 2],\n            [6,  32, 1, 2],\n            [6,  64, 1, 1],\n            [6, 128, 1, 1],\n            [6, 64, 4, 2],\n            [6,  96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # Feature Extractor (MobileNetV2 backbone)\n        input_channel = _make_divisible(32 * width_mult, 8)\n        layers = [conv_3x3_bn(3, input_channel, 2)]\n        block = InvertedResidual\n        for t, c, n, s in self.cfgs:\n            output_channel = _make_divisible(c * width_mult, 8)\n            for i in range(n):\n                layers.append(block(input_channel, output_channel, s if i == 0 else 1, t))\n                input_channel = output_channel\n        self.features = nn.Sequential(*layers)\n\n        # Keypoint Detector Head\n        self.local_head = LocalHead(config['local_head'])\n\n        # Descriptor Head\n        self.global_head = nn.Sequential(\n            VLAD(config['global_head']),\n            DimensionalityReduction(config['global_head'])\n        )\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = image_normalization(x)\n        # Backbone\n        features_1 = self.features[:7](x)\n        features_2 = self.features[7:](features_1)\n\n        # local_head\n        desc, logits, prob_full, prob = self.local_head(features_1)\n\n        # Classification (if needed)\n        descriptor = self.global_head(features_2)\n\n        return {'local_descriptor_map':desc,\n                'logits':logits,\n                'prob_full':prob_full,\n                'scores_dense':prob,\n                'global_descriptor':descriptor,\n                'image_shape':x.shape\n                }\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T09:37:25.577381Z","iopub.execute_input":"2025-02-27T09:37:25.577712Z","iopub.status.idle":"2025-02-27T09:37:25.603886Z","shell.execute_reply.started":"2025-02-27T09:37:25.577685Z","shell.execute_reply":"2025-02-27T09:37:25.603005Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### Test_model","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"config= {\n        'loss_weights': 'uncertainties',\n        'local_head': {\n            'descriptor_dim': 128,\n            'detector_grid': 8,\n            'input_channels': 96\n        },\n        'global_head': {\n            'n_clusters': 32,\n            'intermediate_proj': 0,\n            'dimensionality_reduction': 4096\n        }\n}\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load the initial weights\nstate_dict = torch.load('/kaggle/input/mobilenetv2-0.75/pytorch/default/1/mobilenetv2_0.75-dace9791.pth',\n                        weights_only=False,\n                        map_location=device)\n\n# Create the model\nmodel = HFNet(config, width_mult=0.75)\n\n# Get the model's state_dict\nmodel_dict = model.state_dict()\n\n# Filter out mismatched keys\nfiltered_state_dict = {k: v for k, v in state_dict.items() if k in model_dict and model_dict[k].shape == v.shape}\n\n# Update the model's weights\nmodel_dict.update(filtered_state_dict)\nmodel.load_state_dict(model_dict)\n\n# # Set the model to evaluation mode\n# image = torch.randn(1, 3, 480,640 ) # [B, C, H, W]\n# output = model(image)\n\n# for key, value in output.items():\n#     if key != 'image_shape':\n#         print(key, value.shape)\n#     else:\n#         print(key, value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T09:37:25.728205Z","iopub.execute_input":"2025-02-27T09:37:25.728578Z","iopub.status.idle":"2025-02-27T09:37:26.508815Z","shell.execute_reply.started":"2025-02-27T09:37:25.728542Z","shell.execute_reply":"2025-02-27T09:37:26.507963Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"## Post-processing","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\n# Non-Maximum Suppression (NMS) function to filter out lower-scoring keypoints\n\ndef simple_nms(scores, radius, iterations=3):\n    \"\"\"Performs non-maximum suppression (NMS) on the heatmap using max-pooling.\"\"\"\n    size = 2 * radius + 1  # Kernel size based on radius\n\n    def max_pool(x):\n        return F.max_pool2d(x.unsqueeze(0).unsqueeze(0), kernel_size=size, stride=1, padding=radius).squeeze(0).squeeze(0)\n\n    zeros = torch.zeros_like(scores, device=scores.device)  # Initialize zero tensor on the same device\n    max_mask = scores == max_pool(scores)  # Mask for local maxima\n\n    # Apply NMS iteratively\n    for _ in range(iterations - 1):\n        supp_mask = max_pool(max_mask.float()).bool()\n        supp_scores = torch.where(supp_mask, zeros, scores)\n        new_max_mask = supp_scores == max_pool(supp_scores)\n        max_mask = max_mask | (new_max_mask & ~supp_mask)\n\n    return torch.where(max_mask, scores, zeros)  # Keep maxima, zero out others\n\n# Main prediction function to process keypoints and extract descriptors\n\ndef predict(ret, config):\n    \"\"\"Processes keypoints and extracts descriptors.\"\"\"\n    scores_dense = ret['scores_dense']\n    # Apply NMS if configured\n    if config['local']['nms_radius']:\n        scores_dense = simple_nms(scores_dense, config['local']['nms_radius'])\n\n    batch_size = scores_dense.shape[0]  # Get batch size\n\n    # Initialize lists to collect outputs for each batch element\n    keypoints_list = []\n    scores_list = []\n    descriptors_list = []\n    min_keypoints = float('inf')  # Track the minimum number of keypoints in the batch\n\n    for b in range(batch_size):  # Process each image in the batch\n        # Extract keypoints where scores are above the threshold\n        keypoints = (scores_dense[b] >= config['local']['detector_threshold']).nonzero(as_tuple=False).to(scores_dense.device)\n        scores = scores_dense[b][keypoints[:, 0], keypoints[:, 1]]  # Get corresponding scores\n\n        # Select top-k keypoints based on scores\n        if config['local']['num_keypoints']:\n            k = min(len(scores), config['local']['num_keypoints'])  # Limit to the specified number of keypoints\n            topk_indices = scores.topk(k, largest=True, sorted=True).indices  # Get top-k indices\n            keypoints = keypoints[topk_indices]  # Select top-k keypoints\n            scores = scores[topk_indices]  # Select top-k scores\n            \n        # Get descriptor map for this image and adjust dimensions\n        desc_map = ret['local_descriptor_map'][b].unsqueeze(0).to(scores_dense.device)  # [1, D, H', W']\n        H_desc, W_desc = desc_map.shape[-2:]  # Descriptor map size\n\n        # Unpack original image dimensions directly (assuming consistent size across the batch)\n        _, _, H_img, W_img = ret['image_shape']  # Unpack dimensions from [B, C, H, W]\n\n        # Scale keypoints from image space to descriptor space\n        keypoints_scaled = keypoints.clone().float().to(scores_dense.device)\n        keypoints_scaled[:, 0] *= (H_desc - 1) / (H_img - 1)  # Scale y-coordinates\n        keypoints_scaled[:, 1] *= (W_desc - 1) / (W_img - 1)  # Scale x-coordinates\n\n        # Normalize keypoints to [-1, 1] for grid_sample\n        keypoints_scaled[:, 0] = (keypoints_scaled[:, 0] / (H_desc - 1)) * 2 - 1\n        keypoints_scaled[:, 1] = (keypoints_scaled[:, 1] / (W_desc - 1)) * 2 - 1\n\n        keypoints_scaled = keypoints_scaled.unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, K, 2]\n\n        # Sample descriptors using grid_sample for interpolation\n        local_descriptors = F.grid_sample(desc_map, keypoints_scaled, mode='bilinear', align_corners=True)\n        local_descriptors = local_descriptors.squeeze(2).squeeze(0).transpose(0, 1)  # [K, D]\n\n        # Normalize descriptors to unit length\n        local_descriptors = F.normalize(local_descriptors, p=2, dim=-1)\n\n        # Append results for this batch\n        keypoints_list.append(keypoints)\n        scores_list.append(scores)\n        descriptors_list.append(local_descriptors)\n\n    # Convert lists to tensors for GPU compatibility\n    ret.update({\n        'keypoints': torch.stack(keypoints_list).to(scores_dense.device),\n        'scores': torch.stack(scores_list).to(scores_dense.device),\n        'local_descriptors': torch.stack(descriptors_list).to(scores_dense.device)\n    })\n    \n    return ret\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T09:37:26.510054Z","iopub.execute_input":"2025-02-27T09:37:26.510383Z","iopub.status.idle":"2025-02-27T09:37:26.522094Z","shell.execute_reply.started":"2025-02-27T09:37:26.510349Z","shell.execute_reply":"2025-02-27T09:37:26.521240Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# ret = {\n#     'scores_dense': torch.rand(1,480, 640),\n#     'local_descriptor_map': torch.rand(1,256,60, 80),  # [H', W', D]\n#     'logits': torch.rand(1,65,60, 80),\n#     'prob_full': torch.rand(1,65,60, 80),\n#     'global_descriptor': torch.rand(1,4096),\n#     'image_shape': torch.tensor([1,3, 480, 640])\n# }\n\n# config = {\n#     'local': {\n#         'detector_threshold': 0.005,\n#         'nms_radius': 0,\n#         'num_keypoints': 10000\n#     }\n# }\n# ret = predict(ret, config)\n\n# for k, v in ret.items():\n#     print(f\"{k}: {v.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T09:37:26.523791Z","iopub.execute_input":"2025-02-27T09:37:26.523991Z","iopub.status.idle":"2025-02-27T09:37:26.539057Z","shell.execute_reply.started":"2025-02-27T09:37:26.523974Z","shell.execute_reply":"2025-02-27T09:37:26.538333Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## Losses","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# -------------------------------\n# üîπ Optimized Global Descriptor Loss\n# -------------------------------\ndef descriptor_global_loss(inp, out):\n    \"\"\"Computes L2 loss between input and output global descriptors.\"\"\"\n    d = (inp['global_descriptor'] - out['global_descriptor']) ** 2\n    return d.mean(dim=-1)  # Use mean instead of sum to avoid large gradients\n\n# -------------------------------\n# üîπ Optimized Local Descriptor Loss\n# -------------------------------\ndef descriptor_local_loss(inp, out):\n    \"\"\"Computes L2 loss for local descriptors with aligned dimensions.\"\"\"\n    inp_desc = inp['local_descriptors']\n    out_desc = out['local_descriptors']\n\n    # Trim to smallest sequence length\n    min_size = min(inp_desc.size(1), out_desc.size(1))\n    inp_desc = inp_desc[:, :min_size, :]\n    out_desc = out_desc[:, :min_size, :]\n\n    # Compute squared L2 loss efficiently\n    d = (inp_desc - out_desc).pow(2).mean(dim=(1, 2))  # Mean over keypoints & descriptors\n    return d\n\n# -------------------------------\n# üîπ Optimized Detector Loss\n# -------------------------------\ndef detector_loss(inp, out, config):\n    \"\"\"Computes cross-entropy loss for keypoints or dense scores.\"\"\"\n    logits = out['logits']  # [B, C, H, W]\n    B, C, H, W = logits.shape\n\n    if 'keypoints' in inp:\n        grid_size = config['local_head']['detector_grid']\n\n        # Precompute keypoint heatmaps instead of dynamically creating them\n        keypoints = inp['keypoints']\n        keypoint_map = torch.zeros((B, 1, H * grid_size, W * grid_size), device=logits.device)\n\n        for b in range(B):\n            kp = keypoints[b].long()\n            mask = (0 <= kp[:, 0]) & (kp[:, 0] < keypoint_map.shape[3]) & (0 <= kp[:, 1]) & (kp[:, 1] < keypoint_map.shape[2])\n            valid_kp = kp[mask]  # Filter out invalid keypoints\n            keypoint_map[b, 0, valid_kp[:, 1], valid_kp[:, 0]] = 1.0\n\n        # Downsample the keypoint heatmap\n        patches = F.unfold(keypoint_map, kernel_size=grid_size, stride=grid_size)  # [B, grid_size^2, new_H*new_W]\n        patches = patches.permute(0, 2, 1).reshape(B, -1, grid_size * grid_size)\n\n        # Construct labels efficiently\n        labels = torch.argmax(torch.cat([2 * patches, torch.ones(B, patches.size(1), 1, device=logits.device)], dim=-1), dim=-1)\n\n        # Reshape logits for cross-entropy\n        logits = logits.permute(0, 2, 3, 1).reshape(B, H * W, C)  # [B, H*W, C]\n        loss = F.cross_entropy(logits.reshape(-1, C), labels.reshape(-1).long(), reduction='mean')\n\n    elif 'dense_scores' in inp:\n        labels = inp['dense_scores']\n        logits = logits.permute(0, 2, 3, 1).reshape(B, H * W, C)  # [B, H*W, C]\n        loss = F.cross_entropy(logits.reshape(-1, C), labels.reshape(-1, labels.size(-1)), reduction='mean')\n\n    else:\n        raise ValueError(\"Input must contain 'keypoints' or 'dense_scores'.\")\n\n    return loss\n\n# -------------------------------\n# üîπ Optimized Overall Loss Function\n# -------------------------------\ndef loss(inputs, outputs, config):\n    \"\"\"Computes the total loss as a weighted combination of all components.\"\"\"\n    desc_g = descriptor_global_loss(inputs, outputs).mean()\n    desc_l = descriptor_local_loss(inputs, outputs).mean()\n    detect = detector_loss(inputs, outputs, config)\n\n    # Efficient weighting\n    if config['loss_weights'] == 'uncertainties':\n        logvars = [nn.Parameter(torch.tensor(1.0, device=desc_g.device)) for _ in range(3)]\n        precisions = [torch.exp(-logvar) for logvar in logvars]\n\n        loss = desc_g * precisions[0] + logvars[0]\n        loss += desc_l * precisions[1] + logvars[1]\n        loss += 2 * detect * precisions[2] + logvars[2]\n\n    else:\n        w = config['loss_weights']\n        total = sum(w.values())\n        loss = (w['global_desc'] * desc_g + w['local_desc'] * desc_l + w['detector'] * detect) / total\n\n    return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T09:37:26.540156Z","iopub.execute_input":"2025-02-27T09:37:26.540462Z","iopub.status.idle":"2025-02-27T09:37:26.560016Z","shell.execute_reply.started":"2025-02-27T09:37:26.540440Z","shell.execute_reply":"2025-02-27T09:37:26.559264Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Metric","metadata":{}},{"cell_type":"code","source":"def compute_metrics(outputs, inputs, config, global_loss_fn, local_loss_fn, detector_loss_fn):\n    \"\"\"\n    Compute various loss metrics for the model, with optional uncertainty-based weighting.\n\n    Args:\n        outputs (dict): The model's predictions.\n        inputs (dict): The ground truth or input data for comparison.\n        config (dict): Configuration parameters, including 'loss_weights'.\n        global_loss_fn (function): Custom function to compute global descriptor loss.\n        local_loss_fn (function): Custom function to compute local descriptor loss.\n        detector_loss_fn (function): Custom function to compute detector cross-entropy loss.\n\n    Returns:\n        dict: A dictionary containing computed loss values and optional trainable log-variance weights.\n    \"\"\"\n    # Compute individual losses using custom functions\n    global_desc_l2 = global_loss_fn(inputs, outputs)\n    local_desc_l2 = local_loss_fn(inputs, outputs)\n    detector_crossentropy = detector_loss_fn(inputs, outputs, config)\n\n    # Initialize the metrics dictionary\n    ret = {\n        'global_desc_l2': global_desc_l2,\n        'local_desc_l2': local_desc_l2,\n        'detector_crossentropy': detector_crossentropy,\n    }\n\n    # Add trainable log-variance weights if 'loss_weights' is set to 'uncertainties'\n    if config.get('loss_weights') == 'uncertainties':\n        logvars = []\n        for i in range(3):  # Assuming 3 metrics\n            # Define log-variance as trainable parameters\n            logvar = torch.nn.Parameter(torch.zeros(1))\n            logvars.append(logvar)\n            ret[f'logvar{i}'] = logvar  # Add to the metrics dictionary\n\n    return ret\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T09:37:26.560752Z","iopub.execute_input":"2025-02-27T09:37:26.560997Z","iopub.status.idle":"2025-02-27T09:37:26.578744Z","shell.execute_reply.started":"2025-02-27T09:37:26.560976Z","shell.execute_reply":"2025-02-27T09:37:26.578024Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T09:37:26.579615Z","iopub.execute_input":"2025-02-27T09:37:26.579896Z","iopub.status.idle":"2025-02-27T09:37:26.597647Z","shell.execute_reply.started":"2025-02-27T09:37:26.579864Z","shell.execute_reply":"2025-02-27T09:37:26.596923Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\nclass CustomImageDataset(Dataset):\n    def __init__(self, image_dir, glo_dir, loc_dir, num, num_keypoints, transform=None):\n        self.image_dir = image_dir\n        self.glo_dir = glo_dir\n        self.loc_dir = loc_dir\n        self.num_keypoints = num_keypoints\n        self.image_files = sorted(os.listdir(image_dir))[:num]\n        self.glo_files = sorted(os.listdir(glo_dir))[:num]\n        self.loc_files = sorted(os.listdir(loc_dir))[:num]\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.image_dir, self.image_files[idx])\n        glo_path = os.path.join(self.glo_dir, self.glo_files[idx])\n        loc_path = os.path.join(self.loc_dir, self.loc_files[idx])\n        \n        # Load image\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n\n        # Load global and local descriptors\n        glo = {key: torch.from_numpy(value) for key, value in np.load(glo_path).items()}\n        loc = {key: torch.from_numpy(value) for key, value in np.load(loc_path).items()}\n        \n        scores = loc['scores']\n        keypoints = loc['keypoints']\n        local_descriptors = loc['local_descriptors']\n\n        # Select top-k keypoints\n        k = min(len(scores), self.num_keypoints)\n        if k > 0:\n            topk_indices = scores.topk(k, largest=True, sorted=True).indices\n            keypoints = keypoints[topk_indices]\n            scores = scores[topk_indices]\n            local_descriptors = local_descriptors[topk_indices]\n        else:\n            keypoints = torch.empty((0, 2))  # Empty tensor if no keypoints\n            scores = torch.empty((0,))\n            local_descriptors = torch.empty((0, local_descriptors.shape[-1]))  # [0, D] shape\n\n        inp = {\n            \"global_descriptor\": glo['global_descriptor'],  # ‚ùå No .to(self.device)\n            \"keypoints\": keypoints,\n            \"scores\": scores,\n            \"local_descriptors\": local_descriptors,\n        }\n\n        return image, inp\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T09:37:26.598620Z","iopub.execute_input":"2025-02-27T09:37:26.598851Z","iopub.status.idle":"2025-02-27T09:37:26.614466Z","shell.execute_reply.started":"2025-02-27T09:37:26.598831Z","shell.execute_reply":"2025-02-27T09:37:26.613663Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, random_split\n\n# Define Transformations\ntransform = transforms.Compose([\n    transforms.Resize((480, 640)),\n    transforms.ToTensor(),\n])\n\n# Paths\ndataset_path = '/kaggle/input/google-landmarks-tiny/images'\nglobal_path = '/kaggle/input/global-descriptors/global_descriptors'\nlocal_path = '/kaggle/input/silk-prediction/silk_prediction'\n\n# Dataset Initialization\ndata_set = CustomImageDataset(\n    image_dir=dataset_path, \n    glo_dir=global_path, \n    loc_dir=local_path,  \n    num=1000,  # Number of images\n    num_keypoints=10000,  \n    transform=transform\n)\n\n# Train, Validation, Test Split\nTRAIN_SIZE = 0.8\nVAL_SIZE = 0.1\nBATCH_SIZE = 1\n\ntrain_size = int(TRAIN_SIZE * len(data_set))\nval_size = int(VAL_SIZE * len(data_set))\ntest_size = len(data_set) - train_size - val_size\n\ntorch.manual_seed(42)  # Ensure reproducibility\ntrain_dataset, val_dataset, test_dataset = random_split(data_set, [train_size, val_size, test_size])\n\n# ‚ö° Optimized DataLoaders ‚ö°\nnum_workers = 2  # Use more workers for faster data loading\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n    num_workers=num_workers, pin_memory=True, prefetch_factor=2\n)\n\nval_loader = DataLoader(\n    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n    num_workers=num_workers, pin_memory=True, prefetch_factor=2\n)\n\ntest_loader = DataLoader(\n    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n    num_workers=num_workers, pin_memory=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T09:37:26.615325Z","iopub.execute_input":"2025-02-27T09:37:26.615583Z","iopub.status.idle":"2025-02-27T09:37:26.641125Z","shell.execute_reply.started":"2025-02-27T09:37:26.615551Z","shell.execute_reply":"2025-02-27T09:37:26.640456Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# for image, label in train_loader:\n#     print(image.shape)\n#     for k, v in label.items():\n#         print(k,v.shape)\n#     break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T09:37:26.643027Z","iopub.execute_input":"2025-02-27T09:37:26.643276Z","iopub.status.idle":"2025-02-27T09:37:26.646497Z","shell.execute_reply.started":"2025-02-27T09:37:26.643256Z","shell.execute_reply":"2025-02-27T09:37:26.645715Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## Training ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\nfrom tqdm import tqdm\n\ndef train_model(model, train_loader, val_loader, config, lr=1e-3, patience=5, epochs=20):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    optimizer = optim.RMSprop(model.parameters(), lr=lr)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=patience)\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    log_file = \"training_log.csv\"\n    pd.DataFrame(columns=['epoch', 'global_desc_l2', 'local_desc_l2', 'detector_crossentropy', 'val_loss', 'learning_rate']).to_csv(log_file, index=False)\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        train_loss = 0\n        metrics = {'global_desc_l2': 0, 'local_desc_l2': 0, 'detector_crossentropy': 0}\n        \n        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\"):\n            images = images.to(device)\n            inputs = {k: v.to(device) for k, v in labels.items()}\n\n            optimizer.zero_grad()\n            ret = model(images)\n            outputs = predict(ret, config)\n            \n            # Compute loss metrics using the compute_metrics function\n            loss_dict = compute_metrics(outputs, inputs, config, descriptor_global_loss, descriptor_local_loss, detector_loss)\n\n            # Extract loss values\n            global_desc_l2 = loss_dict['global_desc_l2']\n            local_desc_l2 = loss_dict['local_desc_l2']\n            detector_crossentropy = loss_dict['detector_crossentropy']\n\n            # Compute total loss\n            total_loss = loss(inputs, outputs, config)\n\n            total_loss.backward()\n            optimizer.step()\n            \n            train_loss += total_loss.item()\n            metrics['global_desc_l2'] += global_desc_l2.mean().item()\n            metrics['local_desc_l2'] += local_desc_l2.mean().item()\n            metrics['detector_crossentropy'] += detector_crossentropy.mean().item()\n\n        # Normalize loss metrics\n        num_batches = len(train_loader)\n        for key in metrics:\n            metrics[key] /= num_batches\n        train_loss /= num_batches\n\n        torch.cuda.empty_cache()  # Free unused GPU memory\n        \n        # Validation\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images = images.to(device)\n                inputs = {k: v.to(device) for k, v in labels.items()}\n\n                ret = model(images)\n                outputs = predict(ret, config)\n                \n                # Compute validation loss\n                loss_dict = compute_metrics(outputs, inputs, config, descriptor_global_loss, descriptor_local_loss, detector_loss)\n                total_loss = loss(inputs, outputs, config)\n                \n                val_loss += total_loss.item()\n\n        val_loss /= len(val_loader)\n        scheduler.step(val_loss)\n\n        # Logging\n        lr = scheduler.get_last_lr()[0]  # Retrieve current learning rate\n        print(f\"Epoch {epoch}/{epochs} | Val Loss: {val_loss:.4f}|\"\n              f\"Local Loss: {metrics['local_desc_l2']:.4f} | Detector Loss: {metrics['detector_crossentropy']:.4f}| Global Loss: {metrics['global_desc_l2']:.4f}| LR: {lr:.6f} |\")\n\n        # Save logs\n        pd.DataFrame([[epoch, metrics['global_desc_l2'], metrics['local_desc_l2'], metrics['detector_crossentropy'], val_loss, lr]],\n                     columns=['epoch', 'global_desc_l2', 'local_desc_l2', 'detector_crossentropy', 'val_loss', 'learning_rate']).to_csv(log_file, mode='a', header=False, index=False)\n\n        # Early stopping\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), \"best_model.pth\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"Early stopping triggered.\")\n                break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T09:37:26.647443Z","iopub.execute_input":"2025-02-27T09:37:26.647723Z","iopub.status.idle":"2025-02-27T09:37:26.663281Z","shell.execute_reply.started":"2025-02-27T09:37:26.647693Z","shell.execute_reply":"2025-02-27T09:37:26.662526Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"config={\n    'loss_weights': 'uncertainties',\n    'local':{\n        'detector_threshold': 0.005,\n        'nms_radius': 0,\n        'num_keypoints': 10000\n    },\n    'local_head': {\n        'descriptor_dim': 128,\n        'detector_grid': 8,\n        'input_channels': 96\n    },\n    'global_head': {\n        'n_clusters': 32,\n        'intermediate_proj': 0,\n        'dimensionality_reduction': 4096\n    }\n}\n\ntrain_model(model, train_loader, val_loader, config, lr=1e-3, patience=5, epochs=30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T09:37:26.664028Z","iopub.execute_input":"2025-02-27T09:37:26.664336Z","iopub.status.idle":"2025-02-27T09:43:32.763756Z","shell.execute_reply.started":"2025-02-27T09:37:26.664292Z","shell.execute_reply":"2025-02-27T09:43:32.762527Z"},"_kg_hide-input":true},"outputs":[{"name":"stderr","text":"Epoch 1/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:36<00:00, 21.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30 | Val Loss: 3.8668|Local Loss: 0.0195 | Detector Loss: 1.1335| Global Loss: 0.0004| LR: 0.001000 |\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:36<00:00, 21.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/30 | Val Loss: 4.0000|Local Loss: 0.0194 | Detector Loss: 1.0971| Global Loss: 0.0004| LR: 0.001000 |\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:37<00:00, 21.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/30 | Val Loss: 3.8167|Local Loss: 0.0194 | Detector Loss: 1.0909| Global Loss: 0.0004| LR: 0.001000 |\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:37<00:00, 21.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/30 | Val Loss: 3.7832|Local Loss: 0.0194 | Detector Loss: 1.0870| Global Loss: 0.0004| LR: 0.001000 |\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:37<00:00, 21.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/30 | Val Loss: 3.8141|Local Loss: 0.0194 | Detector Loss: 1.0844| Global Loss: 0.0004| LR: 0.001000 |\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:37<00:00, 21.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/30 | Val Loss: 3.7855|Local Loss: 0.0194 | Detector Loss: 1.0822| Global Loss: 0.0003| LR: 0.001000 |\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:36<00:00, 21.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/30 | Val Loss: 3.7848|Local Loss: 0.0194 | Detector Loss: 1.0804| Global Loss: 0.0003| LR: 0.001000 |\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:37<00:00, 21.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/30 | Val Loss: 3.7846|Local Loss: 0.0194 | Detector Loss: 1.0786| Global Loss: 0.0003| LR: 0.001000 |\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:36<00:00, 21.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/30 | Val Loss: 4.0126|Local Loss: 0.0193 | Detector Loss: 1.0773| Global Loss: 0.0003| LR: 0.001000 |\nEarly stopping triggered.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}