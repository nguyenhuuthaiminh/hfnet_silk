{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10836245,"sourceType":"datasetVersion","datasetId":6729254},{"sourceId":10836295,"sourceType":"datasetVersion","datasetId":6729285},{"sourceId":11117038,"sourceType":"datasetVersion","datasetId":6931859},{"sourceId":266829,"sourceType":"modelInstanceVersion","modelInstanceId":228332,"modelId":250092}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\n\ndef setseed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    \ntorch.cuda.empty_cache()\nsetseed(42)\nprint(\"Seed set successfully\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T02:12:10.960587Z","iopub.execute_input":"2025-03-27T02:12:10.960901Z","iopub.status.idle":"2025-03-27T02:12:12.800458Z","shell.execute_reply.started":"2025-03-27T02:12:10.960876Z","shell.execute_reply":"2025-03-27T02:12:12.799419Z"}},"outputs":[{"name":"stdout","text":"Seed set successfully\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Losses","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef image_normalization(image, pixel_value_offset=128.0, pixel_value_scale=128.0):\n    \"\"\"\n    Normalizes an image tensor by subtracting an offset and dividing by a scale.\n    Typically used to convert [0,255] RGB values into a normalized range.\n    \n    Args:\n        image (torch.Tensor): Input image of shape [C, H, W] or [B, C, H, W].\n        pixel_value_offset (float): Value to subtract from each pixel.\n        pixel_value_scale (float): Value to divide each pixel after offset.\n    \n    Returns:\n        torch.Tensor: Normalized image tensor.\n    \"\"\"\n    return (image - pixel_value_offset) / pixel_value_scale\n\ndef descriptor_global_loss(inp, out):\n    \"\"\"\n    Computes the L2 loss between the input and output global descriptors.\n    \n    Args:\n        inp (dict): Dictionary containing 'global_descriptor' tensor of shape [B, D].\n        out (dict): Dictionary containing 'global_descriptor' tensor of shape [B, D].\n    \n    Returns:\n        torch.Tensor: Scalar L2 loss for each sample in the batch, shape [B].\n    \"\"\"\n    d = torch.square((inp['global_descriptor'] - out['global_descriptor']))\n    return torch.sum(d, dim=-1) / 2\n\ndef descriptor_local_loss(inp, out):\n    \"\"\"\n    Computes the L2 loss for local descriptors, which are spatial feature maps.\n    \n    Args:\n        inp (dict): Contains 'local_descriptor_map' of shape [B, C, H, W].\n        out (dict): Contains 'local_descriptor_map' of shape [B, C, H, W].\n    \n    Returns:\n        torch.Tensor: L2 loss map of shape [B, H, W].\n                     Each pixel's loss is averaged across channels.\n    \"\"\"\n    # print(inp['local_descriptor_map'][:5])\n    # print(out['local_descriptor_map'][:5])\n    d = torch.square(inp['local_descriptor_map'] - out['local_descriptor_map'])\n    # Sum over channels and divide by 2\n    d = torch.sum(d, dim=-1) / 2\n    return d\n\ndef detector_loss(inp, out, config, threshold=0.5):\n    \"\"\"\n    Computes cross-entropy loss for keypoints or dense scores.\n    \n    Args:\n        inp (dict): Must contain either 'keypoint_map' or 'dense_scores'.\n                    - 'keypoint_map' should be [B, 1, H, W] or [B, C, H, W].\n                    - 'dense_scores' should be [B, C, H, W].\n        out (dict): Contains 'logits' of shape [B, C, H, W].\n        config (dict): Configuration dictionary with 'local_head' subkey 'detector_grid'.\n        threshold (float): Not currently used directly in the function, but\n                           included for future modifications (e.g., if you need\n                           to threshold logits or labels).\n    \n    Returns:\n        torch.Tensor: Scalar cross-entropy loss.\n    \"\"\"\n    logits = out['logits']  # [B, C, H, W]\n    B, C, H, W = logits.shape\n    grid_size = config['local_head']['detector_grid']\n\n    if 'keypoint_map' in inp: # Hard Labels\n        labels = inp['keypoint_map']  # e.g. [B, 1, H, W]\n        loss = F.binary_cross_entropy_with_logits(logits, labels)\n\n    elif 'dense_scores' in inp: # Soft labes\n        # If dense_scores is used as a multi-class probability map or similar\n        labels = inp['dense_scores']  # [B, C, H, W]\n        loss = F.cross_entropy(logits, labels)\n\n    else:\n        raise ValueError(\"Input must contain 'keypoint_map' or 'dense_scores'.\")\n\n    return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T02:12:12.801706Z","iopub.execute_input":"2025-03-27T02:12:12.802041Z","iopub.status.idle":"2025-03-27T02:12:12.810595Z","shell.execute_reply.started":"2025-03-27T02:12:12.802019Z","shell.execute_reply":"2025-03-27T02:12:12.809447Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## HF-Net","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom torch.nn import init\n\nclass VLAD(nn.Module):\n    def __init__(self, config):\n        super(VLAD, self).__init__()\n        self.intermediate_proj = config.get('intermediate_proj', None)\n        if self.intermediate_proj:\n            self.pre_proj = nn.Conv2d(240, self.intermediate_proj, kernel_size=1)\n\n        self.n_clusters = config['n_clusters']\n        self.memberships = nn.Conv2d(240, self.n_clusters, kernel_size=1)\n\n        # Cluster centers\n        self.clusters = nn.Parameter(torch.empty(1, self.n_clusters, 240))\n        self._initialize_weights()  # Initialize the cluster weights\n\n    def _initialize_weights(self):\n        # Xavier initialization for cluster weights\n        init.xavier_uniform_(self.clusters)\n\n    def forward(self, feature_map, mask=None):\n        if self.intermediate_proj:\n            feature_map = self.pre_proj(feature_map)\n\n        batch_size, _, h, w = feature_map.size()\n\n        # Compute memberships (soft-assignment)\n        memberships = F.softmax(self.memberships(feature_map), dim=1)\n\n        # Reshape feature_map and clusters for broadcasting\n        feature_map = feature_map.permute(0, 2, 3, 1).unsqueeze(3)  # (B, H, W, 1, D)\n        residuals = self.clusters - feature_map  # Compute residuals\n        residuals = residuals * memberships.permute(0, 2, 3, 1).unsqueeze(4)  # Weight residuals by memberships\n\n        if mask is not None:\n            residuals = residuals * mask.unsqueeze(-1).unsqueeze(-1)\n\n        # Sum residuals to form the VLAD descriptor\n        descriptor = residuals.sum(dim=[1, 2])\n\n        # Intra-normalization\n        descriptor = F.normalize(descriptor, p=2, dim=-1)\n\n        # Flatten descriptor and apply L2 normalization\n        descriptor = descriptor.view(batch_size, -1)\n        descriptor = F.normalize(descriptor, p=2, dim=1)\n\n        return descriptor\n\nclass DimensionalityReduction(nn.Module):\n    def __init__(self, config, proj_regularizer=None):\n        \"\"\"\n        Initializes the Dimensionality Reduction module.\n\n        Args:\n            input_dim (int): Dimension of the input feature descriptor.\n            output_dim (int): Dimension of the reduced descriptor.\n            proj_regularizer (float, optional): L2 regularization strength. If None, no regularization is applied.\n        \"\"\"\n        super(DimensionalityReduction, self).__init__()\n        input_dim = config['n_clusters'] * 240\n        output_dim = config['dimensionality_reduction']\n        self.proj_regularizer = proj_regularizer\n\n        # Fully connected layer with Xavier initialization\n        self.fc = nn.Linear(input_dim, output_dim)\n        nn.init.xavier_uniform_(self.fc.weight)\n\n        # Optional L2 regularization\n        if proj_regularizer is not None:\n            self.regularizer = lambda w: proj_regularizer * torch.sum(w ** 2)\n        else:\n            self.regularizer = None\n\n    def forward(self, descriptor):\n        \"\"\"\n        Forward pass for the Dimensionality Reduction module.\n\n        Args:\n            descriptor (torch.Tensor): Input feature descriptor of shape (batch_size, input_dim).\n\n        Returns:\n            torch.Tensor: Reduced and normalized descriptor of shape (batch_size, output_dim).\n        \"\"\"\n        # Normalize the input descriptor\n        descriptor = F.normalize(descriptor, p=2, dim=-1)\n\n        # Apply the fully connected layer\n        descriptor = self.fc(descriptor)\n\n        # Normalize the output descriptor\n        descriptor = F.normalize(descriptor, p=2, dim=-1)\n\n        # Apply regularization if specified\n        if self.regularizer is not None:\n            reg_loss = self.regularizer(self.fc.weight)\n            return descriptor, reg_loss\n\n        return descriptor\n\nclass LocalHead(nn.Module):\n    def __init__(self, config):\n        super(LocalHead, self).__init__()\n        descriptor_dim = config['descriptor_dim']\n        detector_grid = config['detector_grid']\n\n        # Descriptor Head\n        self.desc_conv1 = nn.Conv2d(config['input_channels'], descriptor_dim, kernel_size=3, stride=1, padding=1)\n        self.desc_bn1 = nn.BatchNorm2d(descriptor_dim)\n        self.desc_conv2 = nn.Conv2d(descriptor_dim, descriptor_dim, kernel_size=1, stride=1, padding=0)\n\n        # Detector Head\n        self.det_conv1 = nn.Conv2d(config['input_channels'], 128, kernel_size=3, stride=1, padding=1)\n        self.det_bn1 = nn.BatchNorm2d(128)\n        self.det_conv2 = nn.Conv2d(128, detector_grid ** 2, kernel_size=1, stride=1, padding=0)\n\n        self.detector_grid = detector_grid\n\n    def forward(self, features):\n        # Descriptor Head\n        desc = F.relu6(self.desc_bn1(self.desc_conv1(features)))\n        desc = self.desc_conv2(desc)\n        desc = F.normalize(desc, p=2, dim=1)\n\n        # Detector Head\n        logits = F.relu6(self.det_bn1(self.det_conv1(features)))\n        logits = self.det_conv2(logits)\n        logits = F.pixel_shuffle(logits, self.detector_grid)\n        \n        prob = F.sigmoid(logits)\n\n        return desc, logits, prob\n\n\ndef _make_divisible(v, divisor, min_value=None):\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\ndef conv_3x3_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        hidden_dim = round(inp * expand_ratio)\n        self.identity = stride == 1 and inp == oup\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n\n    def forward(self, x):\n        if self.identity:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass HFNet(nn.Module):\n    def __init__(self, config, width_mult=1.0):\n        super(HFNet, self).__init__()\n        # [expand_ratio, channels, repeats, stride]\n        self.cfgs = [\n            [1,  16, 1, 1],\n            [6,  24, 2, 2],\n            [6,  32, 1, 2],\n            [6,  64, 1, 1],\n            [6, 128, 1, 1],\n            [6, 64, 4, 2],\n            [6,  96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n        image_channels = config['image_channels']\n        # Feature Extractor (MobileNetV2 backbone)\n        input_channel = _make_divisible(32 * width_mult, 8)\n        layers = [conv_3x3_bn(image_channels, input_channel, 2)]\n        block = InvertedResidual\n        for t, c, n, s in self.cfgs:\n            output_channel = _make_divisible(c * width_mult, 8)\n            for i in range(n):\n                layers.append(block(input_channel, output_channel, s if i == 0 else 1, t))\n                input_channel = output_channel\n        self.features = nn.Sequential(*layers)\n\n        # Keypoint Detector Head\n        self.local_head = LocalHead(config['local_head'])\n\n        # Descriptor Head\n        self.global_head = nn.Sequential(\n            VLAD(config['global_head']),\n            DimensionalityReduction(config['global_head'])\n        )\n        self.logvars = nn.Parameter(torch.tensor([1.0, 1.0, 1.0], requires_grad=True))\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = image_normalization(x)\n        # Backbone\n        features_1 = self.features[:7](x)\n        features_2 = self.features[7:](features_1)\n\n        # local_head\n        desc, logits, prob = self.local_head(features_1)\n\n        # Classification (if needed)\n        descriptor = self.global_head(features_2)\n\n        return {'local_descriptor_map':desc,\n                'logits':logits,\n                'dense_scores':prob,\n                'global_descriptor':descriptor,\n                'image_shape':x.shape\n                }\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n                \n    def _compute_loss(self, inputs, outputs, config):\n        \"\"\"Computes the total loss using external loss functions.\"\"\"\n        desc_g = descriptor_global_loss(inputs, outputs).mean()\n        desc_l = descriptor_local_loss(inputs, outputs).mean()\n        detect = detector_loss(inputs, outputs, config).mean()\n\n        # Apply weighting\n        if config['loss_weights'] == 'uncertainties':\n            w = {f'logvar_{i}': logvar.item() for i, logvar in enumerate(self.logvars)}\n            precisions = [torch.exp(-logvar) for logvar in self.logvars]\n            total_loss = desc_g * precisions[0] + (self.logvars[0])\n            total_loss += desc_l * precisions[1] + (self.logvars[1])\n            total_loss += 2 * detect * precisions[2] + (self.logvars[2])\n        else:\n            w = config['loss_weights']\n            total_loss = (\n                (w['global'] * desc_g + w['local'] * desc_l + w['detector'] * detect) /\n                sum(w.values())\n            )\n        \n\n        return total_loss, {\n            'global_desc_l2': desc_g.item(),\n            'local_desc_l2': desc_l.item(),\n            'detector_crossentropy': detect.item()\n        }, w","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T02:12:12.812504Z","iopub.execute_input":"2025-03-27T02:12:12.812801Z","iopub.status.idle":"2025-03-27T02:12:12.935404Z","shell.execute_reply.started":"2025-03-27T02:12:12.812778Z","shell.execute_reply":"2025-03-27T02:12:12.934263Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"    image = torch.randn(1, 1, 480, 640)\n\n    config= {\n    'image_channels':1,\n    'loss_weights': 'uncertainties',\n    'local':{\n        'detector_threshold': 0.001,\n        'nms_radius': 4,\n        'num_keypoints': 10000\n    },\n    'local_head': {\n        'descriptor_dim': 128,\n        'detector_grid': 8,\n        'input_channels': 96\n    },\n    'global_head': {\n        'n_clusters': 32,\n        'intermediate_proj': 0,\n        'dimensionality_reduction': 4096\n        }\n    }\n    model = HFNet(config, width_mult=0.75)\n\n    ret = model(image)\n    for k, v in ret.items():\n        if k != \"image_shape\":\n            print(k,v.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T02:12:12.937000Z","iopub.execute_input":"2025-03-27T02:12:12.937321Z","iopub.status.idle":"2025-03-27T02:12:14.604422Z","shell.execute_reply.started":"2025-03-27T02:12:12.937286Z","shell.execute_reply":"2025-03-27T02:12:14.602896Z"}},"outputs":[{"name":"stdout","text":"local_descriptor_map torch.Size([1, 128, 60, 80])\nlogits torch.Size([1, 1, 480, 640])\ndense_scores torch.Size([1, 1, 480, 640])\nglobal_descriptor torch.Size([1, 4096])\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### Prediction","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef simple_nms(scores, radius, iterations=3):\n    \"\"\"\n    Performs non-maximum suppression (NMS) on a 2D heatmap using max-pooling.\n\n    Args:\n        scores (torch.Tensor): 2D tensor of shape [H, W] representing the heatmap.\n        radius (int): Neighborhood radius for local maxima.\n        iterations (int): Number of NMS iterations to apply.\n\n    Returns:\n        torch.Tensor: Heatmap with non-maxima suppressed to zero.\n    \"\"\"\n    size = 2 * radius + 1\n\n    def max_pool(x):\n        # Temporarily add batch & channel dims for max_pool2d\n        return F.max_pool2d(x.unsqueeze(0).unsqueeze(0), kernel_size=size, stride=1, padding=radius).squeeze()\n\n    zeros = torch.zeros_like(scores)\n    max_mask = scores == max_pool(scores)\n\n    for _ in range(iterations - 1):\n        suppression_mask = max_pool(max_mask.float()).bool()\n        suppressed_scores = torch.where(suppression_mask, zeros, scores)\n        new_max_mask = suppressed_scores == max_pool(suppressed_scores)\n        max_mask |= new_max_mask & ~suppression_mask\n\n    return torch.where(max_mask, scores, zeros)\n\n\ndef predict(ret, config):\n    \"\"\"\n    Extracts keypoints and local descriptors from model outputs.\n\n    Args:\n        ret (dict): Dictionary containing:\n            - 'dense_scores': [B, 1, H, W] score maps\n            - 'local_descriptor_map': [B, C, H', W'] descriptor maps\n            - 'image_shape': shape info for scaling keypoints\n        config (dict): Configuration dict with 'local' sub-dict specifying\n                       nms_radius, detector_threshold, num_keypoints, etc.\n\n    Returns:\n        dict: Updated 'ret' with new keys:\n              'keypoints', 'scores', 'local_descriptors'\n    \"\"\"\n    dense_scores = ret['dense_scores'].squeeze(1)  # shape: [B, H, W]\n    batch_size = dense_scores.shape[0]\n\n    keypoints_list, scores_list, descriptors_list = [], [], []\n\n    for b in range(batch_size):\n        # Apply NMS if specified\n        scores = dense_scores[b]\n        if config['local'].get('nms_radius', 0) > 0:\n            scores = simple_nms(scores, config['local']['nms_radius'])\n\n        # Threshold keypoints\n        mask = scores >= config['local']['detector_threshold']\n        keypoints = torch.nonzero(mask, as_tuple=False)  # [N, 2]\n        kp_scores = scores[keypoints[:, 0], keypoints[:, 1]]\n\n        # Keep top-k if specified\n        k = config['local'].get('num_keypoints', None)\n        if k is not None and k > 0:\n            k = min(keypoints.shape[0], k)\n            kp_scores, idx = torch.topk(kp_scores, k)\n            keypoints = keypoints[idx]\n\n        # Reshape to [1, N, 2], flip to [x, y]\n        keypoints = keypoints.unsqueeze(0).flip(-1)\n        kp_scores = kp_scores.unsqueeze(0)\n\n        # Prepare descriptor map\n        desc_map = ret['local_descriptor_map'][b].unsqueeze(0)  # [1, C, H', W']\n        _, C, H_desc, W_desc = desc_map.shape\n\n        # Extract original image shape to scale keypoints\n        _, _, H_img, W_img = ret['image_shape']\n        scale = torch.tensor([W_desc - 1, H_desc - 1], device=scores.device, dtype=torch.float32)\n        scale /= torch.tensor([W_img - 1, H_img - 1], device=scores.device, dtype=torch.float32)\n        scale = scale.flip(0)  # [H_scale, W_scale]\n\n        # Scale keypoints to descriptor map size\n        keypoints_scaled = keypoints.float() * scale.unsqueeze(0).unsqueeze(0)\n\n        # Normalize keypoints for grid_sample to [-1, 1]\n        norm_kpts = keypoints_scaled.clone()\n        norm_kpts[..., 0] = (norm_kpts[..., 0] / (W_desc - 1)) * 2 - 1\n        norm_kpts[..., 1] = (norm_kpts[..., 1] / (H_desc - 1)) * 2 - 1\n\n        # Sample descriptors\n        grid = norm_kpts.unsqueeze(2)  # [1, N, 1, 2]\n        local_desc = F.grid_sample(desc_map, grid, align_corners=True)  # [1, C, N, 1]\n        local_desc = local_desc.squeeze(-1).permute(0, 2, 1)           # [1, N, C]\n        local_desc = F.normalize(local_desc, p=2, dim=-1)\n\n        keypoints_list.append(keypoints)\n        scores_list.append(kp_scores)\n        descriptors_list.append(local_desc)\n\n    # Stack results for the batch\n    device = dense_scores.device\n    ret.update({\n        'keypoints': torch.stack(keypoints_list).to(device),\n        'scores': torch.stack(scores_list).to(device),\n        'local_descriptors': torch.stack(descriptors_list).to(device)\n    })\n    \n    return ret\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T02:12:14.605233Z","iopub.execute_input":"2025-03-27T02:12:14.605559Z","iopub.status.idle":"2025-03-27T02:12:14.626640Z","shell.execute_reply.started":"2025-03-27T02:12:14.605525Z","shell.execute_reply":"2025-03-27T02:12:14.625559Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Post-processing","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef desc4silk(ret,out):\n    \"\"\"\n    Upsample `local_descriptor_map` from [B, 128, 60, 80] to [B, 128, 480, 640]\n    using `F.grid_sample()`.\n    out: silk output including descriptor_position \n    \"\"\"\n    # Extract relevant tensors from the input dictionary\n    dense_scores = ret['dense_scores']  # shape: [B, H, W]\n    keypoints = out['keypoints'].long()  # shape: [N, 2]\n    desc_map = ret['local_descriptor_map']  # [1, C, H', W']\n    _, C, H_desc, W_desc = desc_map.shape\n\n    # Extract original image shape to scale keypoints\n    _, _, H_img, W_img = ret['image_shape']\n    scale = torch.tensor([H_desc - 1, W_desc - 1], device=keypoints.device, dtype=torch.float32)\n    scale /= torch.tensor([H_img - 1, W_img - 1], device=keypoints.device, dtype=torch.float32)\n\n    # Scale keypoints to descriptor map size\n    keypoints_scaled = keypoints.float() * scale.unsqueeze(0).unsqueeze(0)\n\n    # Normalize keypoints for grid_sample to [-1, 1]\n    norm_kpts = keypoints_scaled.clone()\n    norm_kpts[..., 0] = (norm_kpts[..., 0] / (W_desc - 1)) * 2 - 1\n    norm_kpts[..., 1] = (norm_kpts[..., 1] / (H_desc - 1)) * 2 - 1\n\n    # Sample descriptors\n    grid = norm_kpts.unsqueeze(2)  # [1, N, 1, 2]\n    local_desc = F.grid_sample(desc_map, grid, align_corners=True)  # [1, C, N, 1]\n    local_desc = local_desc.squeeze(-1).permute(0, 2, 1)          # [1, N, C]\n    local_desc = F.normalize(local_desc, p = 2, dim = -1)\n\n    ret['local_descriptor_map'] = local_desc\n\n    return ret\n\ntorch.manual_seed(42)\n\n# Example usage\nret = {\n    'dense_scores': torch.rand(1, 480, 640),        # shape: [B, H, W]\n    'local_descriptor_map': torch.rand(1, 128, 60, 80),  # shape: [B, 128, H', W']\n    'image_shape': torch.rand(1, 1, 480, 640).shape\n}\nout = {\n    'keypoints': torch.rand((100,2))*100\n}\nprint(out['keypoints'][:5])\nrets = desc4silk(ret,out)\nprint(rets['local_descriptor_map'].shape)  # Expected: [1,\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T02:12:14.627718Z","iopub.execute_input":"2025-03-27T02:12:14.628054Z","iopub.status.idle":"2025-03-27T02:12:14.709717Z","shell.execute_reply.started":"2025-03-27T02:12:14.628016Z","shell.execute_reply":"2025-03-27T02:12:14.708767Z"}},"outputs":[{"name":"stdout","text":"tensor([[79.7267, 36.3293],\n        [71.5480, 14.8105],\n        [77.2471,  7.2567],\n        [67.1530, 34.6592],\n        [16.0746, 62.6228]])\ntorch.Size([1, 100, 128])\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T02:12:14.710698Z","iopub.execute_input":"2025-03-27T02:12:14.711016Z","iopub.status.idle":"2025-03-27T02:12:17.322958Z","shell.execute_reply.started":"2025-03-27T02:12:14.710982Z","shell.execute_reply":"2025-03-27T02:12:17.322032Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\nclass CustomImageDataset(Dataset):\n    \"\"\"\n    A dataset class that:\n      - Loads an image from 'image_dir'\n      - Loads corresponding global and local descriptors from 'glo_dir' and 'loc_dir'\n      - Selects top-k keypoints by score\n      - Constructs descriptor and keypoint maps at the image resolution\n    \"\"\"\n    def __init__(\n        self,\n        image_dir: str,\n        glo_dir: str,\n        loc_dir: str,\n        num: int,\n        num_keypoints: int,\n        transform=None\n    ):\n        \"\"\"\n        Args:\n            image_dir (str): Directory containing images.\n            glo_dir (str): Directory containing global descriptors (.npz files).\n            loc_dir (str): Directory containing local descriptors/keypoints (.npz files).\n            num (int): Number of samples to load.\n            num_keypoints (int): Number of top keypoints to keep based on scores.\n            transform (callable, optional): Transform to be applied on the PIL image.\n        \"\"\"\n        self.image_dir = image_dir\n        self.glo_dir = glo_dir\n        self.loc_dir = loc_dir\n        self.num_keypoints = num_keypoints\n        self.transform = transform\n\n        # Collect file names, sorted for consistent ordering\n        self.image_files = sorted(os.listdir(image_dir))[:num]\n        self.glo_files = sorted(os.listdir(glo_dir))[:num]\n        self.loc_files = sorted(os.listdir(loc_dir))[:num]\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Returns:\n            image (torch.Tensor): The transformed image of shape [C, H, W].\n            inp (dict): Dictionary containing:\n                \"global_descriptor\" (torch.Tensor),\n                \"keypoint_map\" (torch.Tensor of shape [1, H, W]),\n                \"local_descriptor_map\" (torch.Tensor of shape [128, H, W])\n        \"\"\"\n        # File paths\n        img_path = os.path.join(self.image_dir, self.image_files[idx])\n        glo_path = os.path.join(self.glo_dir, self.glo_files[idx])\n        loc_path = os.path.join(self.loc_dir, self.loc_files[idx])\n        \n        # Load and transform the image\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)  # shape: [C, H, W]\n\n        # Load descriptors\n        glo_data = np.load(glo_path)\n        loc_data = np.load(loc_path)\n\n        glo = {k: torch.from_numpy(v) for k, v in glo_data.items()}\n        loc = {k: torch.from_numpy(v) for k, v in loc_data.items()}\n\n        # Extract local data\n        scores = loc['scores'].squeeze(-1)         # shape: [N]\n        keypoints = loc['keypoint']               # shape: [N, 2]\n        local_descriptors = loc['local_descriptor_map']  # shape: [N, 128]\n\n        # Select top-k keypoints\n        topk_indices = scores.topk(self.num_keypoints, largest=True).indices\n        keypoints = keypoints[topk_indices].to(torch.int32)\n        scores = scores[topk_indices]\n        local_descriptors = local_descriptors[topk_indices]\n\n        local_descriptors = F.normalize(local_descriptors, p = 2,dim =-1)\n\n        # Build descriptor & keypoint maps\n        _, H_img, W_img = image.shape  # shape: [C, H, W]\n        keypoint_map = torch.zeros(1, H_img, W_img)\n        \n        # Place scores in the keypoint map\n        keypoint_map[:, keypoints[:, 0], keypoints[:, 1]] = 1.0\n\n        # Build dictionary\n        inp = {\n            \"global_descriptor\": glo['global_descriptor'],\n            \"keypoint_map\": keypoint_map,\n            \"local_descriptor_map\": local_descriptors,\n            \"keypoints\": keypoints\n        }\n\n        return image, inp\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T02:12:17.325519Z","iopub.execute_input":"2025-03-27T02:12:17.325863Z","iopub.status.idle":"2025-03-27T02:12:17.337385Z","shell.execute_reply.started":"2025-03-27T02:12:17.325842Z","shell.execute_reply":"2025-03-27T02:12:17.336410Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import os\nimport torch\nimport random\nimport numpy as np\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, random_split\n\ndef setseed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nsetseed(42)\n\n# --- Transforms ---\ntransform = transforms.Compose([\n    transforms.Resize((480, 640)),\n    transforms.Grayscale(num_output_channels=1),\n    transforms.ColorJitter(\n        brightness=0.2,\n        contrast=0.5\n    ),\n    transforms.ToTensor(),\n])\n\n# --- Paths ---\ndataset_path = '/kaggle/input/google-landmarks-tiny/images'\nglobal_path = '/kaggle/input/global-descriptors/global_descriptors'\nlocal_path = '/kaggle/input/silk-exported/silk_predictions'\n\n# --- Dataset Initialization ---\ndata_set = CustomImageDataset(\n    image_dir=dataset_path,\n    glo_dir=global_path,\n    loc_dir=local_path,\n    num=1000,          # Number of images to load\n    num_keypoints=3000,\n    transform=transform\n)\n\n# --- Train/Val/Test Split ---\nTRAIN_RATIO = 0.8\nVAL_RATIO = 0.2\nBATCH_SIZE = 1\n\ntrain_size = int(TRAIN_RATIO * len(data_set))\nval_size = int(VAL_RATIO * len(data_set))\ntest_size = len(data_set) - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = random_split(\n    data_set,\n    [train_size, val_size, test_size],\n    generator=torch.Generator().manual_seed(42)\n)\n\n# --- Dataloaders ---\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,     \n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T02:12:17.338392Z","iopub.execute_input":"2025-03-27T02:12:17.338617Z","iopub.status.idle":"2025-03-27T02:12:17.518312Z","shell.execute_reply.started":"2025-03-27T02:12:17.338597Z","shell.execute_reply":"2025-03-27T02:12:17.517628Z"},"_kg_hide-input":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"for k, v in data_set:\n    print(k.shape)\n    for k1,v1 in v.items():\n        if k1  == 'local_descriptor_map' or k1 =='keypoint_map' :\n            print(k1,v1.shape)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T02:12:17.519181Z","iopub.execute_input":"2025-03-27T02:12:17.519514Z","iopub.status.idle":"2025-03-27T02:12:17.712814Z","shell.execute_reply.started":"2025-03-27T02:12:17.519482Z","shell.execute_reply":"2025-03-27T02:12:17.712015Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 480, 640])\nkeypoint_map torch.Size([1, 480, 640])\nlocal_descriptor_map torch.Size([3000, 128])\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"for image, label in train_loader:\n    print(image.shape)\n    for k, v in label.items():\n        print(k,v.shape)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T02:12:17.713583Z","iopub.execute_input":"2025-03-27T02:12:17.713826Z","iopub.status.idle":"2025-03-27T02:12:17.819976Z","shell.execute_reply.started":"2025-03-27T02:12:17.713794Z","shell.execute_reply":"2025-03-27T02:12:17.818966Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 1, 480, 640])\nglobal_descriptor torch.Size([1, 4096])\nkeypoint_map torch.Size([1, 1, 480, 640])\nlocal_descriptor_map torch.Size([1, 3000, 128])\nkeypoints torch.Size([1, 3000, 2])\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Training ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\nfrom tqdm import tqdm\n\ndef train_model(\n    model,\n    train_loader,\n    val_loader,\n    config,\n    lr=1e-3,\n    patience=5,\n    epochs=20,\n    log_file=\"training_log.csv\"\n):\n    \"\"\"\n    Trains and validates the model, logging progress to a CSV file.\n\n    Args:\n        model (nn.Module): The model to train.\n        train_loader (DataLoader): DataLoader for training set.\n        val_loader (DataLoader): DataLoader for validation set.\n        config (dict): Configuration dictionary for the model/loss.\n        lr (float): Initial learning rate.\n        patience (int): Number of epochs to wait for improvement before early stopping.\n        epochs (int): Total number of epochs to train.\n        log_file (str): File path for CSV logging.\n\n    Returns:\n        None. The function saves checkpoints to disk.\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    # Optimizer and learning rate scheduler\n    optimizer = optim.RMSprop(model.parameters(), lr=lr)\n    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[epochs], gamma=0.1)\n    \n    # Set up early stopping\n    best_val_loss = float('inf')\n    patience_counter = 0\n    \n    # Prepare CSV logging\n    columns = ['epoch', 'global_desc_l2', 'local_desc_l2', 'detector_crossentropy',\n               'train_loss', 'val_loss', 'learning_rate']\n    pd.DataFrame(columns=columns).to_csv(log_file, index=False)\n    \n    # Main training loop\n    for epoch in range(1, epochs + 1):\n        model.train()\n        train_loss = 0.0\n        metrics = {'global_desc_l2': 0.0, 'local_desc_l2': 0.0, 'detector_crossentropy': 0.0}\n        \n        # ----------------------------\n        #       Training Phase\n        # ----------------------------\n        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} [Train]\"):\n            images = images.to(device)\n            inputs = {k: v.to(device) for k, v in labels.items()}\n    \n            optimizer.zero_grad()\n            \n            # Forward pass\n            ret = model(images)\n            outputs = desc4silk(ret,inputs) # Example function call for post-processing\n            \n            # Compute total loss\n            total_loss, loss_dict, logvar = model._compute_loss(inputs, outputs, config)\n            \n            # Backprop and update\n            total_loss.backward()\n            optimizer.step()\n            \n            # Accumulate training metrics\n            train_loss += total_loss.item()\n            metrics['global_desc_l2'] += loss_dict['global_desc_l2']\n            metrics['local_desc_l2'] += loss_dict['local_desc_l2']\n            metrics['detector_crossentropy'] += loss_dict['detector_crossentropy']\n\n        # Normalize training metrics by number of batches\n        num_batches = len(train_loader)\n        for key in metrics:\n            metrics[key] /= num_batches\n        train_loss /= num_batches\n\n        # Clear unused GPU memory (optional)\n        torch.cuda.empty_cache()\n        \n        # ----------------------------\n        #       Validation Phase\n        # ----------------------------\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch}/{epochs} [Val]\"):\n                images = images.to(device)\n                inputs = {k: v.to(device) for k, v in labels.items()}\n                \n                # Forward pass\n                ret = model(images)\n                outputs = desc4silk(ret,inputs)\n                \n                # Compute validation loss\n                total_loss, loss_dict, _ = model._compute_loss(inputs, outputs, config)\n                val_loss += total_loss.item()\n\n        val_loss /= len(val_loader)\n        \n        # Update learning rate scheduler\n        scheduler.step()\n        current_lr = scheduler.get_last_lr()[0]\n\n        # ----------------------------\n        #       Logging\n        # ----------------------------\n        print(\n            f\"Epoch {epoch}/{epochs} | \"\n            f\"Train Loss: {train_loss:.4f} | \"\n            f\"Val Loss: {val_loss:.4f} | \"\n            f\"Global L2: {metrics['global_desc_l2']:.4f} | \"\n            f\"Local L2: {metrics['local_desc_l2']:.4f} | \"\n            f\"Detector CE: {metrics['detector_crossentropy']:.4f} | \"\n            f\"LR: {current_lr:.6f}\"\n        )\n\n        # (Optional) Log any extra info from logvar\n        if logvar:\n            extra_info = \" | \".join([f\"{k}: {v:.4f}\" for k, v in logvar.items()])\n            print(extra_info)\n\n        # Save metrics to CSV\n        row_data = [\n            epoch,\n            metrics['global_desc_l2'],\n            metrics['local_desc_l2'],\n            metrics['detector_crossentropy'],\n            train_loss,\n            val_loss,\n            current_lr\n        ]\n        df = pd.DataFrame([row_data], columns=columns)\n        df.iloc[:, 1:] = df.iloc[:, 1:].astype(float).round(4)  # Format to 4 decimals\n        df.to_csv(log_file, mode='a', header=False, index=False)\n\n        # ----------------------------\n        #   Early Stopping Check\n        # ----------------------------\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), \"best_model.pth\")\n        else:\n            patience_counter += 1\n            torch.save(model.state_dict(), \"last_model.pth\")\n            if patience_counter >= patience:\n                print(\"Early stopping triggered.\")\n                break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T02:12:17.820946Z","iopub.execute_input":"2025-03-27T02:12:17.821207Z","iopub.status.idle":"2025-03-27T02:12:18.124054Z","shell.execute_reply.started":"2025-03-27T02:12:17.821185Z","shell.execute_reply":"2025-03-27T02:12:18.123332Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"setseed(42)\nconfig= {\n    'image_channels':1,\n    # 'loss_weights': 'uncertainties',\n    'loss_weights':{\n        'global':1,\n        'local':1,\n        'detector':1\n    },\n    'local_head': {\n        'descriptor_dim': 128,\n        'detector_grid': 8,\n        'input_channels': 96\n    },\n    'global_head': {\n        'n_clusters': 32,\n        'intermediate_proj': 0,\n        'dimensionality_reduction': 4096\n    }\n}\nmodel = HFNet(config, width_mult=0.75)\n\ntrain_model(model, train_loader, val_loader, config, lr=1e-3, patience=20, epochs=19)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T02:12:18.125034Z","iopub.execute_input":"2025-03-27T02:12:18.125492Z","iopub.status.idle":"2025-03-27T02:34:39.879021Z","shell.execute_reply.started":"2025-03-27T02:12:18.125468Z","shell.execute_reply":"2025-03-27T02:34:39.877945Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/19 [Train]: 100%|██████████| 800/800 [01:57<00:00,  6.81it/s]\nEpoch 1/19 [Val]: 100%|██████████| 100/100 [00:11<00:00,  8.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/19 | Train Loss: 0.5339 | Val Loss: 0.5322 | Global L2: 0.8151 | Local L2: 0.7342 | Detector CE: 0.0523 | LR: 0.001000\nglobal: 1.0000 | local: 1.0000 | detector: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/19 [Train]: 100%|██████████| 800/800 [01:01<00:00, 13.05it/s]\nEpoch 2/19 [Val]: 100%|██████████| 100/100 [00:04<00:00, 20.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/19 | Train Loss: 0.5255 | Val Loss: 0.5344 | Global L2: 0.8051 | Local L2: 0.7322 | Detector CE: 0.0392 | LR: 0.001000\nglobal: 1.0000 | local: 1.0000 | detector: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/19 [Train]: 100%|██████████| 800/800 [01:00<00:00, 13.19it/s]\nEpoch 3/19 [Val]: 100%|██████████| 100/100 [00:04<00:00, 20.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/19 | Train Loss: 0.5231 | Val Loss: 0.5433 | Global L2: 0.8028 | Local L2: 0.7320 | Detector CE: 0.0345 | LR: 0.001000\nglobal: 1.0000 | local: 1.0000 | detector: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/19 [Train]: 100%|██████████| 800/800 [01:01<00:00, 13.09it/s]\nEpoch 4/19 [Val]: 100%|██████████| 100/100 [00:04<00:00, 20.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/19 | Train Loss: 0.5216 | Val Loss: 0.5417 | Global L2: 0.8016 | Local L2: 0.7317 | Detector CE: 0.0316 | LR: 0.001000\nglobal: 1.0000 | local: 1.0000 | detector: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/19 [Train]: 100%|██████████| 800/800 [01:00<00:00, 13.17it/s]\nEpoch 5/19 [Val]: 100%|██████████| 100/100 [00:04<00:00, 20.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/19 | Train Loss: 0.5205 | Val Loss: 0.5415 | Global L2: 0.7994 | Local L2: 0.7312 | Detector CE: 0.0309 | LR: 0.001000\nglobal: 1.0000 | local: 1.0000 | detector: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/19 [Train]: 100%|██████████| 800/800 [01:00<00:00, 13.25it/s]\nEpoch 6/19 [Val]: 100%|██████████| 100/100 [00:04<00:00, 21.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/19 | Train Loss: 0.5190 | Val Loss: 0.5384 | Global L2: 0.7955 | Local L2: 0.7310 | Detector CE: 0.0306 | LR: 0.001000\nglobal: 1.0000 | local: 1.0000 | detector: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/19 [Train]: 100%|██████████| 800/800 [00:59<00:00, 13.34it/s]\nEpoch 7/19 [Val]: 100%|██████████| 100/100 [00:04<00:00, 20.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/19 | Train Loss: 0.5179 | Val Loss: 0.5407 | Global L2: 0.7920 | Local L2: 0.7307 | Detector CE: 0.0309 | LR: 0.001000\nglobal: 1.0000 | local: 1.0000 | detector: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/19 [Train]: 100%|██████████| 800/800 [00:59<00:00, 13.34it/s]\nEpoch 8/19 [Val]: 100%|██████████| 100/100 [00:04<00:00, 20.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/19 | Train Loss: 0.5196 | Val Loss: 0.5404 | Global L2: 0.7956 | Local L2: 0.7310 | Detector CE: 0.0322 | LR: 0.001000\nglobal: 1.0000 | local: 1.0000 | detector: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/19 [Train]: 100%|██████████| 800/800 [01:00<00:00, 13.19it/s]\nEpoch 9/19 [Val]: 100%|██████████| 100/100 [00:04<00:00, 20.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/19 | Train Loss: 0.5194 | Val Loss: 0.5379 | Global L2: 0.7949 | Local L2: 0.7310 | Detector CE: 0.0322 | LR: 0.001000\nglobal: 1.0000 | local: 1.0000 | detector: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/19 [Train]: 100%|██████████| 800/800 [01:02<00:00, 12.89it/s]\nEpoch 10/19 [Val]: 100%|██████████| 100/100 [00:05<00:00, 19.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/19 | Train Loss: 0.5190 | Val Loss: 0.5637 | Global L2: 0.7937 | Local L2: 0.7308 | Detector CE: 0.0323 | LR: 0.001000\nglobal: 1.0000 | local: 1.0000 | detector: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/19 [Train]: 100%|██████████| 800/800 [01:03<00:00, 12.52it/s]\nEpoch 11/19 [Val]: 100%|██████████| 100/100 [00:05<00:00, 19.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/19 | Train Loss: 0.5187 | Val Loss: 0.5389 | Global L2: 0.7924 | Local L2: 0.7307 | Detector CE: 0.0331 | LR: 0.001000\nglobal: 1.0000 | local: 1.0000 | detector: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/19 [Train]: 100%|██████████| 800/800 [01:04<00:00, 12.48it/s]\nEpoch 12/19 [Val]: 100%|██████████| 100/100 [00:05<00:00, 18.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/19 | Train Loss: 0.5166 | Val Loss: 0.5471 | Global L2: 0.7871 | Local L2: 0.7302 | Detector CE: 0.0323 | LR: 0.001000\nglobal: 1.0000 | local: 1.0000 | detector: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/19 [Train]: 100%|██████████| 800/800 [01:04<00:00, 12.44it/s]\nEpoch 13/19 [Val]: 100%|██████████| 100/100 [00:05<00:00, 18.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/19 | Train Loss: 0.5140 | Val Loss: 0.5404 | Global L2: 0.7802 | Local L2: 0.7300 | Detector CE: 0.0317 | LR: 0.001000\nglobal: 1.0000 | local: 1.0000 | detector: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/19 [Train]: 100%|██████████| 800/800 [01:03<00:00, 12.67it/s]\nEpoch 14/19 [Val]: 100%|██████████| 100/100 [00:05<00:00, 19.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/19 | Train Loss: 0.5129 | Val Loss: 0.5319 | Global L2: 0.7765 | Local L2: 0.7300 | Detector CE: 0.0321 | LR: 0.001000\nglobal: 1.0000 | local: 1.0000 | detector: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/19 [Train]: 100%|██████████| 800/800 [01:01<00:00, 12.99it/s]\nEpoch 15/19 [Val]: 100%|██████████| 100/100 [00:04<00:00, 20.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/19 | Train Loss: 0.5114 | Val Loss: 0.5581 | Global L2: 0.7723 | Local L2: 0.7295 | Detector CE: 0.0323 | LR: 0.001000\nglobal: 1.0000 | local: 1.0000 | detector: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/19 [Train]: 100%|██████████| 800/800 [01:01<00:00, 13.05it/s]\nEpoch 16/19 [Val]: 100%|██████████| 100/100 [00:04<00:00, 20.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/19 | Train Loss: 0.5099 | Val Loss: 0.5478 | Global L2: 0.7686 | Local L2: 0.7294 | Detector CE: 0.0317 | LR: 0.001000\nglobal: 1.0000 | local: 1.0000 | detector: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/19 [Train]: 100%|██████████| 800/800 [01:02<00:00, 12.78it/s]\nEpoch 17/19 [Val]: 100%|██████████| 100/100 [00:05<00:00, 19.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/19 | Train Loss: 0.5083 | Val Loss: 0.5538 | Global L2: 0.7642 | Local L2: 0.7290 | Detector CE: 0.0317 | LR: 0.001000\nglobal: 1.0000 | local: 1.0000 | detector: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/19 [Train]: 100%|██████████| 800/800 [01:03<00:00, 12.60it/s]\nEpoch 18/19 [Val]: 100%|██████████| 100/100 [00:05<00:00, 19.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/19 | Train Loss: 0.5060 | Val Loss: 0.5438 | Global L2: 0.7577 | Local L2: 0.7288 | Detector CE: 0.0315 | LR: 0.001000\nglobal: 1.0000 | local: 1.0000 | detector: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/19 [Train]: 100%|██████████| 800/800 [01:01<00:00, 12.94it/s]\nEpoch 19/19 [Val]: 100%|██████████| 100/100 [00:05<00:00, 19.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/19 | Train Loss: 0.5031 | Val Loss: 0.5790 | Global L2: 0.7488 | Local L2: 0.7289 | Detector CE: 0.0315 | LR: 0.000100\nglobal: 1.0000 | local: 1.0000 | detector: 1.0000\n","output_type":"stream"}],"execution_count":13}]}